{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46bb071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac03446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32498a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_raw_data(year: int, month: int) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the raw data for the specified year and month from the given URL,\n",
    "    extracts CSV files from the ZIP, merges them if multiple, and saves as a single CSV.\n",
    "\n",
    "    Args:\n",
    "        year (int): The year of the data to fetch.\n",
    "        month (int): The month of the data to fetch.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the merged CSV file.\n",
    "    \"\"\"\n",
    "    url = f'https://s3.amazonaws.com/tripdata/{year}{month:02}-citibike-tripdata.csv.zip'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'{url} is not available')\n",
    "\n",
    "    raw_data_dir = Path('..') / 'data' / 'raw'\n",
    "    raw_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Path to save the combined data as parquet\n",
    "    output_file_path = raw_data_dir / f'citi_bike_rides_{year}_{month:02}.parquet'\n",
    "\n",
    "    try:\n",
    "        expected_columns = ['ride_id',\n",
    "                            'rideable_type',\n",
    "                            'started_at',\n",
    "                            'ended_at',\n",
    "                            'start_station_name',\n",
    "                            'start_station_id',\n",
    "                            'end_station_name',\n",
    "                            'end_station_id',\n",
    "                            'start_lat',\n",
    "                            'start_lng',\n",
    "                            'end_lat',\n",
    "                            'end_lng',\n",
    "                            'member_casual']\n",
    "        \n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:\n",
    "            csv_files = [file for file in zip_file.namelist() if file.endswith('.csv')]\n",
    "\n",
    "            if not csv_files:\n",
    "                raise Exception(f'No CSV files found in {url}')\n",
    "\n",
    "            dataframes = []\n",
    "            for file in csv_files:\n",
    "                with zip_file.open(file) as file:\n",
    "                    data_df = pd.read_csv(file, encoding = 'latin1', on_bad_lines = 'skip')\n",
    "                    data_df = data_df[[col for col in expected_columns if col in data_df.columns]]\n",
    "                    dataframes.append(data_df)\n",
    "\n",
    "            merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "            # Drop NaNs if exist\n",
    "            merged_df = merged_df.dropna()\n",
    "\n",
    "            # Clean the data: Convert start_station_id and end_station_id to strings\n",
    "            merged_df['start_station_id'] = merged_df['start_station_id'].astype(str)\n",
    "            merged_df['end_station_id'] = merged_df['end_station_id'].astype(str)\n",
    "\n",
    "            # Convert started_at and ended_at to datetime\n",
    "            merged_df['started_at'] = pd.to_datetime(merged_df['started_at'], errors='coerce')\n",
    "            merged_df['ended_at'] = pd.to_datetime(merged_df['ended_at'], errors='coerce')\n",
    "\n",
    "            # Ensure other columns are of appropriate types\n",
    "            merged_df['start_lat'] = merged_df['start_lat'].astype(float, errors='ignore')\n",
    "            merged_df['start_lng'] = merged_df['start_lng'].astype(float, errors='ignore')\n",
    "            merged_df['end_lat'] = merged_df['end_lat'].astype(float, errors='ignore')\n",
    "            merged_df['end_lng'] = merged_df['end_lng'].astype(float, errors='ignore')\n",
    "            merged_df['member_casual'] = merged_df['member_casual'].astype(str)\n",
    "\n",
    "            # Inspect the DataFrame after cleaning\n",
    "            print('\\nDataFrame info after cleaning:')\n",
    "            print(merged_df.info())\n",
    "\n",
    "            # Save the combined DataFrame as a parquet file\n",
    "            merged_df.to_parquet(output_file_path, engine = 'pyarrow', index = False)\n",
    "            print(f'Successfully fetched and saved: {str(output_file_path)}')\n",
    "            return str(output_file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f'Error processing data from {url}: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e430ea06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/10/l7cxtpss5ygbsz2r9p4pc_jw0000gn/T/ipykernel_21665/420891319.py:49: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_df = pd.read_csv(file, encoding = 'latin1', on_bad_lines = 'skip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame info after cleaning:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1881977 entries, 0 to 1888084\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   ride_id             object        \n",
      " 1   rideable_type       object        \n",
      " 2   started_at          datetime64[ns]\n",
      " 3   ended_at            datetime64[ns]\n",
      " 4   start_station_name  object        \n",
      " 5   start_station_id    object        \n",
      " 6   end_station_name    object        \n",
      " 7   end_station_id      object        \n",
      " 8   start_lat           float64       \n",
      " 9   start_lng           float64       \n",
      " 10  end_lat             float64       \n",
      " 11  end_lng             float64       \n",
      " 12  member_casual       object        \n",
      "dtypes: datetime64[ns](2), float64(4), object(7)\n",
      "memory usage: 201.0+ MB\n",
      "None\n",
      "Successfully fetched and saved: ../data/raw/citi_bike_rides_2024_01.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/raw/citi_bike_rides_2024_01.parquet'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_raw_data(2024, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
